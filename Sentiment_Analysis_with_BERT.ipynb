{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis with BERT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB7G-UQNNyfD"
      },
      "source": [
        "This notebook needs to be run from Google Colab!! If viewing the file from github, click on the button below to open this file in Golab and execute this code!\r\n",
        "\r\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dilipis/CourseProject/blob/main/Sentiment_Analysis_with_BERT.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYmZ0Z9-Tlhc"
      },
      "source": [
        "This code needs to run on a GPU. From Colab, navigate to **Edit> Notebook Settings**. Select **GPU** from the 'Hardware accelerator' dropdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_di-QRDOy6q"
      },
      "source": [
        "The data for training and testing the model is stored in a github repository. The code below copies it to the workspace in Colab for easier processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Lxici7vxE6"
      },
      "source": [
        "import os\n",
        "\n",
        "source_folder = './data'\n",
        "output_folder = './output'\n",
        "github_source = 'https://raw.githubusercontent.com/dilipis/CourseProject/main/data/'\n",
        "\n",
        "# Folder for storing the source files in Colab\n",
        "os.makedirs(os.path.dirname(source_folder + '/dummy.txt'), exist_ok=True)\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Copy the source files from github into the workspace\n",
        "urllib.request.urlretrieve(github_source + '/test.jsonl', source_folder +'/test.jsonl')\n",
        "urllib.request.urlretrieve(github_source + '/train.jsonl', source_folder +'/train.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_g8C3tOPeNQ"
      },
      "source": [
        "The twitter data needs to be read from the jsonl files and preprocessed. This block of code does the following\r\n",
        "\r\n",
        "\r\n",
        "1.   Read the testing and training data from jsonl file and convert them into a csv file\r\n",
        "2.    Clean the input data by removing URL and USER tags from the tweets\r\n",
        "3.    Split the training dataset into training and validation. This will be used to train the model.\r\n",
        "4.    Extract only the required columns for further processing. Only the label field and the response field are currently being used.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAkEX1pJnIZK"
      },
      "source": [
        "# Contains BERT modules that we would be using\n",
        "!pip install transformers\n",
        "\n",
        "# For converting the jsonl file into the required format\n",
        "!pip install jsonlines\n",
        "\n",
        "# The training set in train.jsonl will be divided into training and validation sets based on this ratio\n",
        "train_valid_ratio = 0.6\n",
        "\n",
        "# Import dependencies\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "import tensorflow as tf\n",
        "import jsonlines\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_data = []\n",
        "\n",
        "# Read the training data and convert to a csv. The SARCASM/NOT_SARCASM values are converted into 0/1\n",
        "with open(source_folder + '/train.jsonl',\"r+\",encoding='utf-8') as input_file:\n",
        "    for tweet in jsonlines.Reader(input_file):\n",
        "        if tweet['label'] == 'SARCASM':\n",
        "            tweet['label'] = 0\n",
        "        else:\n",
        "            tweet['label'] = 1\n",
        "\n",
        "        input_data.append(tweet)\n",
        "\n",
        "# Create the folder for storing transformed data if it does not exist \n",
        "file_path = output_folder + \"/twitter.csv\"\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# The tweets in the training data need to be preprocessed and cleaned. The following items are removed - USER tags and URL tags. The data is then stored temporarily in a csv\n",
        "with open(file_path, 'w+', newline='', encoding='utf-8') as input_file:\n",
        "    csv_writer = csv.writer(input_file)\n",
        "    csv_head = ['label','response', 'context', 'ID']\n",
        "    csv_writer.writerow(csv_head)\n",
        "\n",
        "    for i in range(len(input_data)):\n",
        "        data_row = [ input_data[i]['label'],  (input_data[i]['response']).replace('@USER', '').replace('<URL>',''),\n",
        "                    (''.join(input_data[i]['context'])).replace('@USER', '').replace('<URL>',''), '']\n",
        "        csv_writer.writerow(data_row)\n",
        "\n",
        "# Read the data into a Pandas dataframe for easier processing\n",
        "tweets = pd.read_csv(output_folder + '/twitter.csv')\n",
        "\n",
        "# Split the input training data into two different dataframes for training and validation \n",
        "tweets_train, tweets_val = train_test_split(tweets, test_size=train_valid_ratio, random_state=42)\n",
        "\n",
        "tweets_train.to_csv(output_folder + '/train.csv', index=False)\n",
        "tweets_val.to_csv(output_folder + '/valid.csv', index=False)\n",
        "\n",
        "# Helper function to read data from a jsonl file\n",
        "def parse_json(file):\n",
        "    for l in open(file,'r'):\n",
        "        yield json.loads(l)\n",
        "\n",
        "file_path = source_folder + \"/test.jsonl\"\n",
        "input_data = list(parse_json(file_path))    \n",
        "\n",
        "# Preprocess test dataset by removing USER and URL tags \n",
        "with open(output_folder + '/test.csv', 'w+', newline='', encoding='utf-8') as input_file:\n",
        "    csv_writer = csv.writer(input_file)\n",
        "    csv_head = ['label','response', 'context', 'ID']\n",
        "    csv_writer.writerow(csv_head)\n",
        "\n",
        "    for i in range(len(input_data)):\n",
        "        data_row = ['3', (input_data[i]['response']).replace('@USER', '').replace('<URL>',''),\n",
        "                    (''.join(input_data[i]['context'])).replace('@USER', '').replace('<URL>',''),input_data[i]['id']]\n",
        "        csv_writer.writerow(data_row)\n",
        "\n",
        "tweets_test = pd.read_csv(output_folder + '/test.csv')\n",
        "\n",
        "# Extract only the required columns from the input data. The context field could be used in the future to achieve better accuracy \n",
        "tweets_train = pd.DataFrame({'response': tweets_train['response'],\n",
        "    'label': tweets_train['label']})\n",
        "\n",
        "tweets_val = pd.DataFrame({'response': tweets_val['response'],\n",
        "    'label': tweets_val['label']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a03GlNUxRTi9"
      },
      "source": [
        "The following code defines the helper functions convert the data into a format that BERT requires for its processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQMYwGY3mbMs"
      },
      "source": [
        "# Helper function to convert the data in the format that BERT expects. \n",
        "# The GUID and text_b are set to None as we are not using it in this implementation\n",
        "def convert_data_to_examples(train, test, response, label): \n",
        "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[response], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[label]), axis = 1)\n",
        "\n",
        "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[response], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[label]), axis = 1)\n",
        "  \n",
        "  return train_InputExamples, validation_InputExamples\n",
        "\n",
        "# Helper function that tokenizes the InputExample object and creates a dataset that can be loaded into the module   \n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = [] # -> will hold InputFeatures to be converted later\n",
        "\n",
        "    for e in examples:\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length, # truncates if len(s) > max_length\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True, # pads to the right by default\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "response = 'response'\n",
        "label = 'label'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9GR0k7mRpFK"
      },
      "source": [
        "This is where the main acion happens! This block is expected to take 5-10 minutes to execute. In this block, we\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "1. Create the BERT model and tokenizer\r\n",
        "2.   Convert the training and validation data into the BERT format using the helper functions defined above\r\n",
        "3. Use model.compile to set the optimizer, loss function that BERT will use to train the model\r\n",
        "4. Call model.fit to actually train the model based on the training and validation data\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embk0_1UqBLi"
      },
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Convert the train and validation data into the format that BERT can process. This uses the helper functions that were defined above.\n",
        "tweets_train_InputExamples, tweets_val_InputExamples = convert_data_to_examples(tweets_train, tweets_val, response, label)\n",
        "\n",
        "tweets_train_data = convert_examples_to_tf_dataset(list(tweets_train_InputExamples), tokenizer)\n",
        "tweets_train_data = tweets_train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "tweets_val_data = convert_examples_to_tf_dataset(list(tweets_val_InputExamples), tokenizer)\n",
        "tweets_val_data = tweets_val_data.batch(32)\n",
        "\n",
        "# Setting up callbacks for TensorBoard\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "\n",
        "# Set the optimizer, loss function and the metrics to track\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "\n",
        "# This trains the model and gets all parameters to get all the paramters to the correct value to map our inputs to our outputs\n",
        "model.fit(tweets_train_data, \n",
        "          epochs=2, \n",
        "          batch_size = 32,\n",
        "          validation_data=tweets_val_data,\n",
        "          callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ0L6bP2Sq9c"
      },
      "source": [
        "Now it is time to do predictions based on our trained model. For each row in the testing dataset, the predictions are obtained. Finally, the resuts are written to answer.txt in the 'output' folder in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RCGrAlY-ISj"
      },
      "source": [
        "\r\n",
        "test_batch = tokenizer(tweets_test['response'].astype(str).tolist(), max_length=64, padding=True, truncation=True, return_tensors='tf')\r\n",
        "tf_outputs = model(test_batch)\r\n",
        "\r\n",
        "# Get the pedictions\r\n",
        "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\r\n",
        "\r\n",
        "#Check if the  prediction is positive(NOT_SARCASM) or negative(SARCASM)\r\n",
        "label = tf.argmax(tf_predictions, axis=1)\r\n",
        "label = label.numpy()\r\n",
        "\r\n",
        "# Will be used in the last step to convert 0/1 to SARCASM?NOT_SARCASM\r\n",
        "labels = ['SARCASM','NOT_SARCASM']\r\n",
        "\r\n",
        "file_path = output_folder + \"/answer.txt\"\r\n",
        "\r\n",
        "# Loop through the predictions and write the result in a text file. This works because tweet_test and labels are the same size and ordered the same\r\n",
        "with open(file_path, 'w+', encoding='utf-8') as results_file:\r\n",
        "    for i in range(len(tweets_test)):\r\n",
        "      results_file.write(tweets_test['ID'][i] + \",\" + labels[label[i]] + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}